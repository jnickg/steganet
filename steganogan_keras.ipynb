{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jnickg/steganet/blob/main/steganogan_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYnJJYUxbdua"
      },
      "source": [
        "# SteganoGAN in Keras\r\n",
        "This notebook contains code attempting to reimplement SteganoGAN in Keras, for the purpose of better understanding (and scrutinizing) it.\r\n",
        "\r\n",
        "*Based on https://github.com/DAI-Lab/SteganoGAN/tree/master/steganogan*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start with the basic `Conv2D` layer, which is used in vairous parts of the overall model. Batch normalization comes after the activation, as seen [here](https://github.com/DAI-Lab/SteganoGAN/blob/master/steganogan/encoders.py#L117-L121)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sub-networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "QbnEM8Oubduh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.layers import BatchNormalization\r\n",
        "from tensorflow.keras.layers import Conv2D\r\n",
        "from tensorflow.keras.layers import LeakyReLU\r\n",
        "\r\n",
        "def steganogan_conv2d_layer(layer_in, num_filters, kernel_size, name=None, normalize=True, activation_fn=LeakyReLU()):\r\n",
        "    model = Conv2D(num_filters, kernel_size, padding='same', activation=activation_fn, name=name)(layer_in)\r\n",
        "    if normalize:\r\n",
        "        normalize_name = f'{name}_normalize' if name is not None else None\r\n",
        "        model = BatchNormalization(name=normalize_name)(model)\r\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoder\r\n",
        "We will focus on the Dense variant of SteganoGAN, since researchers reported getting the best results with hit. This takes four hyperparameters: The image dimensions $(W, H, C)$, and the depth of the data to be encoded $D$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "H2mhSvCYbdui"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input\r\n",
        "from tensorflow.keras.layers import Concatenate\r\n",
        "from tensorflow.keras.layers import Add\r\n",
        "from tensorflow.keras.activations import sigmoid\r\n",
        "\r\n",
        "def steganogan_encoder_dense_model(W, H, C, D):\r\n",
        "    \"\"\"\r\n",
        "    The BasicEncoder module takes an cover image and a data tensor and combines\r\n",
        "    them into a steganographic image.\r\n",
        "    Input: (N, 3, H, W), (N, D, H, W)\r\n",
        "    Output: (N, 3, H, W)\r\n",
        "    \"\"\"\r\n",
        "    input_image = Input(shape=(W, H, C), name=f'image{W}x{H}x{C}')\r\n",
        "    input_data  = Input(shape=(W, H, D), name=f'data{W}x{H}x{D}')\r\n",
        "\r\n",
        "    image_preprocess = steganogan_conv2d_layer(input_image, 32, 3, name='image_preprocess')\r\n",
        "\r\n",
        "    image_data_process_1_in = Concatenate(name='image_data_process_1_in')([image_preprocess, input_data])\r\n",
        "    image_data_process_1 = steganogan_conv2d_layer(image_data_process_1_in, 32, 3, name='image_data_process_1')\r\n",
        "\r\n",
        "    image_data_process_2_in = Concatenate(name='image_data_process_2_in')([image_preprocess, image_data_process_1, input_data])\r\n",
        "    image_data_process_2 = steganogan_conv2d_layer(image_data_process_2_in, 32, 3, name='image_data_process_2')\r\n",
        "\r\n",
        "    encoder_in = Concatenate(name='encoder_in')([image_preprocess, image_data_process_1, image_data_process_2, input_data])\r\n",
        "    encoder = steganogan_conv2d_layer(encoder_in, 3, 3, name='encoder', normalize=False, activation_fn=sigmoid)\r\n",
        "\r\n",
        "    encoder_out = Add(name='encoder_out')([input_image, encoder])\r\n",
        "\r\n",
        "    return ([input_image, input_data], encoder_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decoder\r\n",
        "\r\n",
        "Similarly, the decoder takes the same hyperparameters. We simplify usage a bit by reshaping the output to be a 1D vector of the decoded data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "kkUkzdhUbduj"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Reshape\r\n",
        "\r\n",
        "def steganogan_decoder_dense_model(W, H, C, D, input=None):\r\n",
        "    \"\"\"\r\n",
        "    The DenseDecoder module takes an steganographic image and attempts to decode\r\n",
        "    the embedded data tensor.\r\n",
        "    Input: (N, 3, H, W)\r\n",
        "    Output: (N, D, H, W)\r\n",
        "    \"\"\"\r\n",
        "    if input is None:\r\n",
        "        input = Input(shape=(W, H, C), name=f'cover_image{W}x{H}x{C}')\r\n",
        "    decode_1 = steganogan_conv2d_layer(input, 32, 3, name='decode_1')\r\n",
        "\r\n",
        "    decode_2 = steganogan_conv2d_layer(decode_1, 32, 3, name='decode_2')\r\n",
        "\r\n",
        "    decode_3_in = Concatenate(name='decode_3_in')([decode_1, decode_2])\r\n",
        "    decode_3 = steganogan_conv2d_layer(decode_3_in, 32, 3, name='decode_3')\r\n",
        "\r\n",
        "    decoder_in = Concatenate(name='decoder_in')([decode_1, decode_2, decode_3])\r\n",
        "    decoder = steganogan_conv2d_layer(decoder_in, D, 3, name='decoder', normalize=False, activation_fn=sigmoid)\r\n",
        "    decoder = Reshape((1,-1))(decoder)\r\n",
        "\r\n",
        "    return input, decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Critic\r\n",
        "\r\n",
        "The critic also takes the same hyperparameters, and produces a single-element tensor as its output: the average of the final convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "1r2ozQ7mbduj"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import AveragePooling2D\r\n",
        "\r\n",
        "def steganogan_critic_model(W, H, C, D):\r\n",
        "    \"\"\"\r\n",
        "    The BasicCritic module takes an image and predicts whether it is a cover\r\n",
        "    image or a steganographic image (N, 1).\r\n",
        "    Input: (N, 3, H, W)\r\n",
        "    Output: (N, 1)\r\n",
        "    \"\"\"\r\n",
        "    model_in = Input(shape=(W, H, C), name=f'image{W}x{H}x{C}')\r\n",
        "    model = steganogan_conv2d_layer(model_in, 32, 3, name='conv_1')\r\n",
        "    model = steganogan_conv2d_layer(model, 32, 3, name='conv_2')\r\n",
        "    model = steganogan_conv2d_layer(model, 32, 3, name='conv_3')\r\n",
        "    model = steganogan_conv2d_layer(model, 1, 3, name='conv_4', normalize=False, activation_fn=sigmoid)\r\n",
        "    model = AveragePooling2D(pool_size=(model.shape[1], model.shape[2]), name='mean')(model)\r\n",
        "    return model_in, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overall Model\r\n",
        "\r\n",
        "We implement the overall model as a subclass of `keras.Model`, which makes it easier to train. It is initialized with the inputs and outputs for each subnetwork, plus the hyperparameters used to create those. Finally, there is a `noise_func`, which is a data generator function. This allows the model to generate user-specified random data, for encoding/decoding steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\r\n",
        "\r\n",
        "class KerasSteganoGAN(tf.keras.Model):\r\n",
        "  def __init__(self, gen_in, gen_out, dec_in, dec_out, critic_in, critic_out, image_height, image_width, image_channels, data_depth, noise_func=None):\r\n",
        "    super(KerasSteganoGAN, self).__init__()\r\n",
        "\r\n",
        "    # Metadata regarding discrminator input images\r\n",
        "    self.img_rows = image_height\r\n",
        "    self.img_cols = image_width\r\n",
        "    self.channels = image_channels\r\n",
        "    self.img_shape = (self.img_rows, self.img_cols, self.channels)\r\n",
        "    \r\n",
        "    # Metadata regarding generator inputdata\r\n",
        "    self.data_depth = data_depth\r\n",
        "    self.noise_func = noise_func\r\n",
        "\r\n",
        "    # Build and compile the critic\r\n",
        "    self.critic = Model(inputs=critic_in, outputs=critic_out, name='KerasSteganoGAN_critic')\r\n",
        "\r\n",
        "    # Build the generator\r\n",
        "    self.encoder = Model(inputs=gen_in, outputs=gen_out, name='KerasSteganoGAN_encoder')\r\n",
        "    self.decoder = Model(inputs=dec_in, outputs=dec_out, name='KerasSteganoGAN_decoder')\r\n",
        "    \r\n",
        "  def compile(self,\r\n",
        "              d_optimizer=Adam(learning_rate=2e-4, beta_1=0.5),\r\n",
        "              g_optimizer=Adam(learning_rate=2e-4, beta_1=0.5),\r\n",
        "              loss=BinaryCrossentropy(),\r\n",
        "              disc_noise_in=(0.0, 4e-2),\r\n",
        "              metrics=['loss']):\r\n",
        "    super(KerasSteganoGAN, self).compile()\r\n",
        "    self.loss_fn = loss\r\n",
        "    self.d_optimizer = d_optimizer\r\n",
        "    self.g_optimizer = g_optimizer\r\n",
        "    self.disc_noise_in=disc_noise_in\r\n",
        "\r\n",
        "  def generator_noise(self, batch_size=1):\r\n",
        "    return noise_func(batch_size, data_depth)\r\n",
        "\r\n",
        "  def critic_loss(self, real_output, fake_output):\r\n",
        "    real_labels = tf.zeros_like(real_output)\r\n",
        "    fake_labels = tf.ones_like(fake_output)\r\n",
        "    # Calculate loss comparing real outputs (\"not fake\") to zeros, and fake\r\n",
        "    # outputs (\"yes fake\") to ones.\r\n",
        "    real_loss = self.loss_fn(real_labels, real_output)\r\n",
        "    fake_loss = self.loss_fn(fake_labels, fake_output)\r\n",
        "    total_loss = real_loss + fake_loss\r\n",
        "    return total_loss\r\n",
        "\r\n",
        "  def generator_loss(self, fake_output):\r\n",
        "    # Calculate loss assuming that they should have been evaluated as real, i.e.\r\n",
        "    # zero, or \"not fake\"\r\n",
        "    return self.loss_fn(tf.zeros_like(fake_output), fake_output)\r\n",
        "\r\n",
        "  def call(self, input_tensor, training=False):\r\n",
        "    x = input_tensor\r\n",
        "    x = self.critic(x, training=training)\r\n",
        "    return x\r\n",
        "\r\n",
        "  def train_step(self, real_images):\r\n",
        "    # If we were give an (x,y) tuple, drop the labels because we just don't care\r\n",
        "    if isinstance(real_images, tuple):\r\n",
        "      real_images = real_images[0]\r\n",
        "\r\n",
        "    # We need to know how big our batches are, so we can create sets of\r\n",
        "    # generated images in the same quantity\r\n",
        "    batch_size = tf.shape(real_images)[0]\r\n",
        "\r\n",
        "    # Train critic first, with real images AND a batch of fakes. Its loss\r\n",
        "    # is determined by how well it can discern between real and fake images\r\n",
        "    seeds = self.encoder_noise(batch_size=batch_size)\r\n",
        "\r\n",
        "    #gen_inputs = \r\n",
        "    generated_images = self.encoder(seeds, training=True)\r\n",
        "\r\n",
        "    # If specified, apply noise to prevent critic from cheating\r\n",
        "    if (self.disc_noise_in is not None):\r\n",
        "      (noise_mean, noise_sd) = self.disc_noise_in\r\n",
        "      noise_shape = [batch_size, self.img_rows, self.img_cols, self.channels]\r\n",
        "      real_images = real_images + tf.random.normal(noise_shape, mean=noise_mean, stddev=noise_sd)\r\n",
        "      generated_images = generated_images + tf.random.normal(noise_shape, mean=noise_mean, stddev=noise_sd)\r\n",
        "\r\n",
        "    with tf.GradientTape() as disc_tape:\r\n",
        "      real_predictions = self.critic(real_images, training=True)\r\n",
        "      fake_predictions = self.critic(generated_images, training=True)\r\n",
        "\r\n",
        "      disc_loss = self.critic_loss(real_predictions, fake_predictions)\r\n",
        "\r\n",
        "    disc_grad = disc_tape.gradient(disc_loss,\r\n",
        "                                   self.critic.trainable_weights)\r\n",
        "    self.d_optimizer.apply_gradients(zip(disc_grad,\r\n",
        "                                         self.critic.trainable_weights))\r\n",
        "\r\n",
        "    # Train generator by pitting it against the updated critic. Its loss\r\n",
        "    # is determined by how well it can trick the critic.\r\n",
        "    gen_seeds = self.encoder_noise(batch_size=batch_size)\r\n",
        "    with tf.GradientTape() as gen_tape:\r\n",
        "      new_generated_images = self.encoder(gen_seeds, training=True)\r\n",
        "      predictions_on_gen = self.critic(new_generated_images, training=True)\r\n",
        "      gen_loss = self.encoder_loss(predictions_on_gen)\r\n",
        "      \r\n",
        "    gen_grad = gen_tape.gradient(gen_loss,\r\n",
        "                                 self.encoder.trainable_weights)\r\n",
        "    self.g_optimizer.apply_gradients(zip(gen_grad,\r\n",
        "                                         self.encoder.trainable_weights))\r\n",
        "    \r\n",
        "\r\n",
        "    return {'gen_loss': gen_loss, 'disc_loss': disc_loss}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameters\r\n",
        "We list out the hyperparameters here, so they are consolidated into a single space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image dimensions\r\n",
        "my_W = 128\r\n",
        "my_H = 128\r\n",
        "my_C = 3\r\n",
        "# Data dimension (along with my_W and my_H)\r\n",
        "my_D = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Generator\r\n",
        "\r\n",
        "Let's define a simple function we can use to generate data for the encoder to put into carrier images. We'll start by using a lorem ipsum generator, and later use it with a custom dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's a sample usage of the API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lorem ipsum.\n"
          ]
        }
      ],
      "source": [
        "from loremipsum import generate_sentences\r\n",
        "\r\n",
        "sentences = generate_sentences(1, start_with_lorem=True)\r\n",
        "for s in sentences:\r\n",
        "  print(s[2])\r\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then, we bake it into a function with a simple adapter interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_data(quantity, depth):\r\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we put it all together and create our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF18XAt_bdui",
        "outputId": "b11d2d8c-a499-49ed-eb60-e8fab1972733"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\r\n",
        "\r\n",
        "def build_steganogan(subnet_summary=False):\r\n",
        "  encoder_in, encoder_out = steganogan_encoder_dense_model(my_W, my_H, my_C, my_D)\r\n",
        "  steganogan_encoder = Model(encoder_in, encoder_out, name='steganogan_encoder')\r\n",
        "  if (subnet_summary):\r\n",
        "    steganogan_encoder.summary()\r\n",
        "\r\n",
        "  decoder_in, decoder_out = steganogan_decoder_dense_model(my_W, my_H, my_C, my_D)\r\n",
        "  steganogan_decoder = Model(decoder_in, decoder_out, name='steganogan_decoder')\r\n",
        "  if (subnet_summary):\r\n",
        "    steganogan_decoder.summary()\r\n",
        "\r\n",
        "  critic_in, critic_out = steganogan_critic_model(my_W, my_H, my_C, my_D)\r\n",
        "  steganogan_critic = Model(critic_in, critic_out, name='steganogan_critic')\r\n",
        "  if (subnet_summary):\r\n",
        "    steganogan_critic.summary()\r\n",
        "\r\n",
        "  steganoGAN = KerasSteganoGAN(encoder_in, encoder_out, decoder_in, decoder_out, critic_in, critic_out, my_H, my_W, my_C, my_D)\r\n",
        "  steganoGAN.build((None, my_H, my_W, my_C))\r\n",
        "  steganoGAN.summary()\r\n",
        "  return steganoGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Actually creating it prints out the parameters for each sub-network, and then prints out a final summary of the consolidated model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"steganogan_encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "image128x128x3 (InputLayer)     [(None, 128, 128, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "image_preprocess (Conv2D)       (None, 128, 128, 32) 896         image128x128x3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "image_preprocess_normalize (Bat (None, 128, 128, 32) 128         image_preprocess[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "data128x128x2 (InputLayer)      [(None, 128, 128, 2) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "image_data_process_1_in (Concat (None, 128, 128, 34) 0           image_preprocess_normalize[0][0] \n",
            "                                                                 data128x128x2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "image_data_process_1 (Conv2D)   (None, 128, 128, 32) 9824        image_data_process_1_in[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "image_data_process_1_normalize  (None, 128, 128, 32) 128         image_data_process_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "image_data_process_2_in (Concat (None, 128, 128, 66) 0           image_preprocess_normalize[0][0] \n",
            "                                                                 image_data_process_1_normalize[0]\n",
            "                                                                 data128x128x2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "image_data_process_2 (Conv2D)   (None, 128, 128, 32) 19040       image_data_process_2_in[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "image_data_process_2_normalize  (None, 128, 128, 32) 128         image_data_process_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_in (Concatenate)        (None, 128, 128, 98) 0           image_preprocess_normalize[0][0] \n",
            "                                                                 image_data_process_1_normalize[0]\n",
            "                                                                 image_data_process_2_normalize[0]\n",
            "                                                                 data128x128x2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Conv2D)                (None, 128, 128, 3)  2649        encoder_in[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "encoder_out (Add)               (None, 128, 128, 3)  0           image128x128x3[0][0]             \n",
            "                                                                 encoder[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 32,793\n",
            "Trainable params: 32,601\n",
            "Non-trainable params: 192\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"steganogan_decoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cover_image128x128x3 (InputLaye [(None, 128, 128, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "decode_1 (Conv2D)               (None, 128, 128, 32) 896         cover_image128x128x3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decode_1_normalize (BatchNormal (None, 128, 128, 32) 128         decode_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "decode_2 (Conv2D)               (None, 128, 128, 32) 9248        decode_1_normalize[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "decode_2_normalize (BatchNormal (None, 128, 128, 32) 128         decode_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "decode_3_in (Concatenate)       (None, 128, 128, 64) 0           decode_1_normalize[0][0]         \n",
            "                                                                 decode_2_normalize[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "decode_3 (Conv2D)               (None, 128, 128, 32) 18464       decode_3_in[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "decode_3_normalize (BatchNormal (None, 128, 128, 32) 128         decode_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "decoder_in (Concatenate)        (None, 128, 128, 96) 0           decode_1_normalize[0][0]         \n",
            "                                                                 decode_2_normalize[0][0]         \n",
            "                                                                 decode_3_normalize[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Conv2D)                (None, 128, 128, 2)  1730        decoder_in[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 1, 32768)     0           decoder[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 30,722\n",
            "Trainable params: 30,530\n",
            "Non-trainable params: 192\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"steganogan_critic\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "image128x128x3 (InputLayer)  [(None, 128, 128, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv_1 (Conv2D)              (None, 128, 128, 32)      896       \n",
            "_________________________________________________________________\n",
            "conv_1_normalize (BatchNorma (None, 128, 128, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv_2 (Conv2D)              (None, 128, 128, 32)      9248      \n",
            "_________________________________________________________________\n",
            "conv_2_normalize (BatchNorma (None, 128, 128, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv_3 (Conv2D)              (None, 128, 128, 32)      9248      \n",
            "_________________________________________________________________\n",
            "conv_3_normalize (BatchNorma (None, 128, 128, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv_4 (Conv2D)              (None, 128, 128, 1)       289       \n",
            "_________________________________________________________________\n",
            "mean (AveragePooling2D)      (None, 1, 1, 1)           0         \n",
            "=================================================================\n",
            "Total params: 20,065\n",
            "Trainable params: 19,873\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n",
            "Model: \"keras_stegano_gan_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "KerasSteganoGAN_critic (Func (None, 1, 1, 1)           20065     \n",
            "_________________________________________________________________\n",
            "KerasSteganoGAN_encoder (Fun (None, 128, 128, 3)       32793     \n",
            "_________________________________________________________________\n",
            "KerasSteganoGAN_decoder (Fun (None, 1, 32768)          30722     \n",
            "=================================================================\n",
            "Total params: 83,580\n",
            "Trainable params: 83,004\n",
            "Non-trainable params: 576\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "steganoGAN = build_steganogan(subnet_summary=True)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function displays a batch of images arranged into a grid, for instrumenting the training of our model. We'll be able to see a few test images along the way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import matplotlib.gridspec as gridspec\r\n",
        "from math import sqrt\r\n",
        "\r\n",
        "def display_batch(images, count, vmin=None, vmax=None):\r\n",
        "  w = int(sqrt(count))\r\n",
        "  f = plt.figure(figsize=(w, w))\r\n",
        "  f.set_size_inches(w+2, w+2)\r\n",
        "  gs1 = gridspec.GridSpec(w, w)\r\n",
        "  gs1.update(wspace=0.025, hspace=0.05)\r\n",
        "  for i in range(count):\r\n",
        "    current_image = np.array(images[i, :, :, 0])\r\n",
        "    # define subplot\r\n",
        "    plt.subplot(w, w, 1 + i)\r\n",
        "    # turn off axis\r\n",
        "    plt.axis('off')\r\n",
        "    # plot raw pixel data\r\n",
        "    plt.imshow(current_image, cmap='gray', vmin=vmin, vmax=vmax)\r\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The class below was created for a normal GAN, but serves as a callback to display batches of test images (using the above function), throughout training.\r\n",
        "\r\n",
        "It will need some modification to support SteganoGAN, whose architecture does not totally align with a standard GAN (i.e. not just \"latent noise\" as the generator input; an actual image is needed too)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GanSampler(keras.callbacks.Callback):\r\n",
        "  def __init__(self, print_seed=False, sample_square_width=1, epoch_interval=10):\r\n",
        "    super(GanSampler, self).__init__()\r\n",
        "    self.print_seed = print_seed\r\n",
        "    self.num_samples = sample_square_width * sample_square_width\r\n",
        "    self.w = sample_square_width\r\n",
        "    self.epoch_interval = epoch_interval\r\n",
        "\r\n",
        "  def display_fixed_seeds(self):\r\n",
        "    generated_images = self.model.generator(self.fixed_seed)\r\n",
        "    display_batch(generated_images, self.num_samples)\r\n",
        "\r\n",
        "  def on_train_begin(self, logs=None):\r\n",
        "    self.fixed_seed = self.model.generator_noise(batch_size=self.num_samples)\r\n",
        "\r\n",
        "    if self.print_seed:\r\n",
        "      print(f'\\nFixed seeds for sampling: {self.fixed_seed}')\r\n",
        "    \r\n",
        "    print('\\nGenerated images using fixed seeds, before ANY training:')\r\n",
        "    self.display_fixed_seeds()\r\n",
        "\r\n",
        "  def on_train_end(self, logs=None):\r\n",
        "    print('\\nFinal generated images using fixed seeds')\r\n",
        "    self.display_fixed_seeds()\r\n",
        "  \r\n",
        "  def on_epoch_end(self, epoch, logs=None):\r\n",
        "    if (epoch % self.epoch_interval == 0):\r\n",
        "      print(f'\\nGenerated image at end of epoch {epoch+1}, using fixed seeds')\r\n",
        "      self.display_fixed_seeds()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "steganogan_keras.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.1 64-bit (conda)",
      "name": "python391jvsc74a57bd07d704ea16be1f842a0e496abdad6f3afdbae930fc438ffef0a0d5019c690fcb5"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 0
}